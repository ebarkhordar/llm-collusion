{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Code Style Classifier\n",
        "\n",
        "Train a model to identify which LLM wrote a piece of code based on coding style, patterns, and conventions.\n",
        "\n",
        "**Purpose**: Given code from different LLMs, the classifier learns to distinguish between them based on:\n",
        "- Docstring style and formatting\n",
        "- Type hints usage\n",
        "- Naming conventions\n",
        "- Error handling patterns\n",
        "- Code structure and idioms\n",
        "\n",
        "**Steps**:\n",
        "1. Load all code samples from different LLMs in the training directory\n",
        "2. Fine-tune a model to classify code authorship\n",
        "3. Use the trained model to identify which LLM wrote specific code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (optional; uncomment if needed)\n",
        "# %pip install -q transformers accelerate datasets torch pyyaml tqdm rich\n",
        "\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import json\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config - edit these\n",
        "INPUT_DATA_DIR = \"../data/code_generation/mbpp-sanitized/train\"  # directory with model-generated code\n",
        "DATASET_FILTER = \"mbpp\"  # filter by benchmark name, or None for all\n",
        "\n",
        "# Model settings\n",
        "BASE_MODEL = \"unsloth/Llama-3.2-1B-Instruct\"  # or \"meta-llama/Llama-3.2-3B-Instruct\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "OUTPUT_DIR = \"../outputs/code_style_classifier\"\n",
        "\n",
        "# Training hyperparameters\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 4\n",
        "GRAD_ACCUM_STEPS = 4\n",
        "LEARNING_RATE = 2e-5\n",
        "MAX_LENGTH = 1024\n",
        "WARMUP_RATIO = 0.1\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# Data split\n",
        "SEED = 42\n",
        "TRAIN_RATIO = 0.8\n",
        "EVAL_RATIO = 0.1  # remaining will be test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utilities\n",
        "\n",
        "def read_jsonl(path: str | Path) -> Iterable[Dict[str, Any]]:\n",
        "    \"\"\"Read JSONL file and yield parsed records.\"\"\"\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        return iter(())\n",
        "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            yield json.loads(line)\n",
        "\n",
        "\n",
        "def format_code_for_classification(task_prompt: str, code: str) -> str:\n",
        "    \"\"\"Format the input for the classifier: task description + code to analyze.\"\"\"\n",
        "    template = (\n",
        "        \"Analyze the coding style of the following code snippet.\\n\\n\"\n",
        "        \"Task: {task_prompt}\\n\\n\"\n",
        "        \"Code:\\n{code}\\n\\n\"\n",
        "        \"Which model wrote this code?\"\n",
        "    )\n",
        "    return template.format(task_prompt=task_prompt.strip(), code=code.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare dataset\n",
        "\n",
        "# Discover all model files\n",
        "input_dir = Path(INPUT_DATA_DIR)\n",
        "jsonl_files = sorted(input_dir.glob(\"*.jsonl\"))\n",
        "print(f\"üìÅ Discovered {len(jsonl_files)} model files in {INPUT_DATA_DIR}:\")\n",
        "for f in jsonl_files:\n",
        "    print(f\"   ‚Ä¢ {f.name}\")\n",
        "\n",
        "# Load all code samples\n",
        "samples = []\n",
        "norm_filter = DATASET_FILTER.strip().lower() if DATASET_FILTER else None\n",
        "\n",
        "for jsonl_file in jsonl_files:\n",
        "    for record in read_jsonl(jsonl_file):\n",
        "        benchmark = str(record.get(\"benchmark\", \"\")).strip()\n",
        "        if norm_filter and benchmark.lower() != norm_filter:\n",
        "            continue\n",
        "        \n",
        "        samples.append({\n",
        "            \"task_prompt\": str(record.get(\"prompt\", \"\")),\n",
        "            \"code\": str(record.get(\"generated_code\", \"\")),\n",
        "            \"model_name\": str(record.get(\"model_name\", \"\")),\n",
        "            \"task_id\": str(record.get(\"task_id\", \"\")),\n",
        "            \"benchmark\": benchmark\n",
        "        })\n",
        "\n",
        "print(f\"\\nüìä Loaded {len(samples)} code samples\")\n",
        "\n",
        "# Build model vocabulary (label mapping)\n",
        "unique_models = sorted(set(s[\"model_name\"] for s in samples))\n",
        "model2id = {model: idx for idx, model in enumerate(unique_models)}\n",
        "id2model = {idx: model for model, idx in model2id.items()}\n",
        "\n",
        "print(f\"\\nü§ñ Models to classify ({len(unique_models)}):\")\n",
        "for model, idx in model2id.items():\n",
        "    count = sum(1 for s in samples if s[\"model_name\"] == model)\n",
        "    print(f\"   [{idx}] {model}: {count} samples\")\n",
        "\n",
        "# Create training examples\n",
        "examples = []\n",
        "for sample in samples:\n",
        "    text = format_code_for_classification(sample[\"task_prompt\"], sample[\"code\"])\n",
        "    label = model2id[sample[\"model_name\"]]\n",
        "    examples.append({\n",
        "        \"text\": text,\n",
        "        \"label\": label,\n",
        "        \"model_name\": sample[\"model_name\"]\n",
        "    })\n",
        "\n",
        "# Split into train/eval/test\n",
        "random.seed(SEED)\n",
        "random.shuffle(examples)\n",
        "\n",
        "n = len(examples)\n",
        "train_end = int(n * TRAIN_RATIO)\n",
        "eval_end = int(n * (TRAIN_RATIO + EVAL_RATIO))\n",
        "\n",
        "train_examples = examples[:train_end]\n",
        "eval_examples = examples[train_end:eval_end]\n",
        "test_examples = examples[eval_end:]\n",
        "\n",
        "print(f\"\\nüìà Dataset split:\")\n",
        "print(f\"   ‚Ä¢ Train: {len(train_examples)} samples\")\n",
        "print(f\"   ‚Ä¢ Eval:  {len(eval_examples)} samples\")\n",
        "print(f\"   ‚Ä¢ Test:  {len(test_examples)} samples\")\n",
        "\n",
        "# Convert to HuggingFace datasets\n",
        "train_ds = Dataset.from_list(train_examples)\n",
        "eval_ds = Dataset.from_list(eval_examples)\n",
        "test_ds = Dataset.from_list(test_examples)\n",
        "\n",
        "# Save model mapping for later use\n",
        "import pickle\n",
        "output_dir = Path(OUTPUT_DIR)\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "with open(output_dir / \"model_mapping.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\"model2id\": model2id, \"id2model\": id2model}, f)\n",
        "print(f\"\\nüíæ Saved model mapping to {output_dir / 'model_mapping.pkl'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare model and tokenizer\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "print(\"üîß Loading tokenizer and model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# For sequence classification, we need a model with a classification head\n",
        "num_labels = len(model2id)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    num_labels=num_labels,\n",
        "    problem_type=\"single_label_classification\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Loaded {BASE_MODEL}\")\n",
        "print(f\"   ‚Ä¢ Classifier head: {num_labels} classes\")\n",
        "print(f\"   ‚Ä¢ Max sequence length: {MAX_LENGTH}\")\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "    )\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"\\nüîÑ Tokenizing datasets...\")\n",
        "train_tokenized = train_ds.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\", \"model_name\"]\n",
        ")\n",
        "eval_tokenized = eval_ds.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\", \"model_name\"]\n",
        ")\n",
        "test_tokenized = test_ds.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\", \"model_name\"]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Tokenization complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the classifier\n",
        "\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "\n",
        "# Data collator for dynamic padding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    \n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average='weighted', zero_division=0\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "    }\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=eval_tokenized,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Starting training...\")\n",
        "print(f\"   ‚Ä¢ Epochs: {EPOCHS}\")\n",
        "print(f\"   ‚Ä¢ Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   ‚Ä¢ Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"   ‚Ä¢ Training samples: {len(train_tokenized)}\")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"\\n‚úÖ Training complete!\")\n",
        "print(f\"üíæ Model saved to: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "\n",
        "print(\"\\nüìä Evaluating on test set...\")\n",
        "test_results = trainer.predict(test_tokenized)\n",
        "\n",
        "# Get predictions\n",
        "predictions = np.argmax(test_results.predictions, axis=1)\n",
        "labels = test_results.label_ids\n",
        "\n",
        "# Overall metrics\n",
        "print(f\"\\n‚ú® Test Set Results:\")\n",
        "print(f\"   ‚Ä¢ Accuracy: {accuracy_score(labels, predictions):.4f}\")\n",
        "\n",
        "# Per-class metrics\n",
        "print(f\"\\nüìà Per-Model Classification Report:\")\n",
        "print(classification_report(\n",
        "    labels, \n",
        "    predictions,\n",
        "    target_names=[id2model[i] for i in range(len(id2model))],\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# Confusion insights\n",
        "print(\"\\nüîç Confusion Analysis:\")\n",
        "for true_label in range(len(id2model)):\n",
        "    true_model = id2model[true_label]\n",
        "    mask = labels == true_label\n",
        "    if mask.sum() == 0:\n",
        "        continue\n",
        "    \n",
        "    preds_for_model = predictions[mask]\n",
        "    correct = (preds_for_model == true_label).sum()\n",
        "    total = mask.sum()\n",
        "    \n",
        "    print(f\"   ‚Ä¢ {true_model}:\")\n",
        "    print(f\"      Correct: {correct}/{total} ({100*correct/total:.1f}%)\")\n",
        "    \n",
        "    # Show most common misclassifications\n",
        "    if correct < total:\n",
        "        wrong_preds = preds_for_model[preds_for_model != true_label]\n",
        "        if len(wrong_preds) > 0:\n",
        "            unique, counts = np.unique(wrong_preds, return_counts=True)\n",
        "            top_confusion = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)[:2]\n",
        "            print(f\"      Often confused with: {', '.join([id2model[int(i)] for i, _ in top_confusion])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference example: Compare two code snippets\n",
        "\n",
        "def classify_code(task_prompt: str, code: str, model, tokenizer, model2id, id2model):\n",
        "    \"\"\"Classify which LLM likely wrote the given code.\"\"\"\n",
        "    text = format_code_for_classification(task_prompt, code)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
        "    \n",
        "    # Move to same device as model\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits, dim=-1)[0]\n",
        "    \n",
        "    # Get top predictions\n",
        "    top_probs, top_indices = torch.topk(probs, k=min(3, len(model2id)))\n",
        "    \n",
        "    results = []\n",
        "    for prob, idx in zip(top_probs.cpu().numpy(), top_indices.cpu().numpy()):\n",
        "        results.append({\n",
        "            \"model\": id2model[int(idx)],\n",
        "            \"probability\": float(prob),\n",
        "            \"confidence\": float(prob * 100)\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def compare_two_codes(task_prompt: str, code1: str, code2: str, model, tokenizer, model2id, id2model):\n",
        "    \"\"\"Compare two code snippets and identify which model likely wrote each.\"\"\"\n",
        "    print(f\"üîç Analyzing code snippets for task:\\n   '{task_prompt[:100]}...'\\n\")\n",
        "    \n",
        "    # Classify code 1\n",
        "    results1 = classify_code(task_prompt, code1, model, tokenizer, model2id, id2model)\n",
        "    print(f\"üìù Code 1 predictions:\")\n",
        "    for r in results1:\n",
        "        print(f\"   ‚Ä¢ {r['model']}: {r['confidence']:.1f}%\")\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # Classify code 2\n",
        "    results2 = classify_code(task_prompt, code2, model, tokenizer, model2id, id2model)\n",
        "    print(f\"üìù Code 2 predictions:\")\n",
        "    for r in results2:\n",
        "        print(f\"   ‚Ä¢ {r['model']}: {r['confidence']:.1f}%\")\n",
        "    \n",
        "    print()\n",
        "    print(f\"üí° Most likely:\")\n",
        "    print(f\"   Code 1 ‚Üí {results1[0]['model']} ({results1[0]['confidence']:.1f}%)\")\n",
        "    print(f\"   Code 2 ‚Üí {results2[0]['model']} ({results2[0]['confidence']:.1f}%)\")\n",
        "    \n",
        "    return results1, results2\n",
        "\n",
        "\n",
        "# Example usage with test data\n",
        "if len(test_ds) >= 2:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üéØ EXAMPLE: Comparing two code snippets\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Get two different samples\n",
        "    sample1 = test_ds[0]\n",
        "    sample2 = test_ds[1]\n",
        "    \n",
        "    # Use the same task prompt for fair comparison\n",
        "    task_prompt = sample1[\"task_prompt\"]\n",
        "    code1 = sample1[\"text\"].split(\"Code:\\n\")[1].split(\"\\n\\nWhich model\")[0]\n",
        "    code2 = sample2[\"text\"].split(\"Code:\\n\")[1].split(\"\\n\\nWhich model\")[0]\n",
        "    \n",
        "    results1, results2 = compare_two_codes(\n",
        "        task_prompt, code1, code2,\n",
        "        model, tokenizer, model2id, id2model\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n‚úÖ Actual labels:\")\n",
        "    print(f\"   Code 1 was written by: {sample1['model_name']}\")\n",
        "    print(f\"   Code 2 was written by: {sample2['model_name']}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
