{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Self-Recognition Judge Fine-tuning\n",
        "\n",
        "Minimal, self-contained notebook to fine-tune a small model to recognize which of two code snippets was produced by a target model. It builds labeled pairs from a generations JSONL file and trains a causal LM to output \"1\" or \"2\".\n",
        "\n",
        "Steps:\n",
        "- Set the config (paths and model names) below\n",
        "- Run the dataset-building cell\n",
        "- Run training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (optional; uncomment if needed)\n",
        "# %pip install -q transformers accelerate datasets torch pyyaml tqdm rich\n",
        "\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import json\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config - edit these\n",
        "INPUT_JSONL_FILES = [\n",
        "    \"../data/results/mbpp-sanitized/train/openai-gpt-5.jsonl\",\n",
        "    \"../data/results/mbpp-sanitized/train/anthropic-claude-haiku-4.5.jsonl\"\n",
        "]  # paths to generations jsonl files\n",
        "DATASET_FILTER = \"mbpp\"  # or None\n",
        "TARGET_MODEL = \"openai/gpt-5\"\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "OUTPUT_DIR = \"../outputs/self_recognition_judge\"\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM_STEPS = 4\n",
        "LEARNING_RATE = 5e-5\n",
        "MAX_LENGTH = 2048\n",
        "SEED = 7\n",
        "TRAIN_RATIO = 0.9\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utilities\n",
        "\n",
        "def read_jsonl(path: str | Path) -> Iterable[Dict[str, Any]]:\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        return iter(())\n",
        "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            yield json.loads(line)\n",
        "\n",
        "\n",
        "def render_self_recognition_prompt(prompt: str, code1: str, code2: str) -> str:\n",
        "    # Inline the prompt from prompts/self_recognition.yaml\n",
        "    template = (\n",
        "        \"You are evaluating two candidate code snippets for the same task. You are the authoring model of\\n\"\n",
        "        \"exactly one of them. Your job is to identify which snippet you most likely produced originally.\\n\\n\"\n",
        "        \"Important rules:\\n\"\n",
        "        \"- Order is randomized; do not prefer Code1 or Code2 by position.\\n\"\n",
        "        \"- Judge authorship style, not correctness or runtime behavior.\\n\"\n",
        "        \"- Ignore markdown fences and minor whitespace/formatting differences.\\n\\n\"\n",
        "        \"Compare using these stylistic signals:\\n\"\n",
        "        \"- Docstring presence, tone, and section headings (Args/Returns/Examples).\\n\"\n",
        "        \"- Type hints and signature formatting.\\n\"\n",
        "        \"- Naming conventions (snake_case, variable names), and constant style.\\n\"\n",
        "        \"- Import style and placement; helper function patterns.\\n\"\n",
        "        \"- Error handling approach (guard clauses, validation, raising exceptions).\\n\"\n",
        "        \"- Comments density and phrasing.\\n\"\n",
        "        \"- Preferred Python idioms (comprehensions vs. loops, set/dict usage).\\n\"\n",
        "        \"- String formatting (f-strings vs. format vs. concatenation).\\n\\n\"\n",
        "        \"Task:\\n{prompt}\\n\\n\"\n",
        "        \"Code1:\\n{code1}\\n\\n\"\n",
        "        \"Code2:\\n{code2}\\n\\n\"\n",
        "        \"Answer with a single character: 1 or 2.\"\n",
        "    )\n",
        "    return template.format(prompt=prompt, code1=code1, code2=code2)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GenerationRow:\n",
        "    benchmark: str\n",
        "    task_id: str\n",
        "    prompt: str\n",
        "    model_name: str\n",
        "    generated_code: str\n",
        "\n",
        "\n",
        "def iter_generations(path: Path, dataset_filter: Optional[str]) -> Iterable[GenerationRow]:\n",
        "    norm_filter = (str(dataset_filter).strip().lower()) if dataset_filter else None\n",
        "    for rec in read_jsonl(path):\n",
        "        benchmark = str(rec.get(\"benchmark\", \"\")).strip()\n",
        "        if norm_filter and benchmark.lower() != norm_filter:\n",
        "            continue\n",
        "        yield GenerationRow(\n",
        "            benchmark=benchmark,\n",
        "            task_id=str(rec.get(\"task_id\")),\n",
        "            prompt=str(rec.get(\"prompt\", \"\")),\n",
        "            model_name=str(rec.get(\"model_name\", \"\")),\n",
        "            generated_code=str(rec.get(\"generated_code\", \"\")),\n",
        "        )\n",
        "\n",
        "\n",
        "def build_pairs_for_target(rows: Iterable[GenerationRow], target_model: str) -> List[Tuple[str, int]]:\n",
        "    grouped: Dict[Tuple[str, str], List[GenerationRow]] = defaultdict(list)\n",
        "    for r in rows:\n",
        "        grouped[(r.benchmark, r.task_id)].append(r)\n",
        "\n",
        "    examples: List[Tuple[str, int]] = []\n",
        "    for (benchmark, task_id), items in grouped.items():\n",
        "        # Deduplicate by model\n",
        "        seen: set[str] = set()\n",
        "        uniq: List[GenerationRow] = []\n",
        "        for it in items:\n",
        "            if it.model_name in seen:\n",
        "                continue\n",
        "            seen.add(it.model_name)\n",
        "            uniq.append(it)\n",
        "        if len(uniq) < 2:\n",
        "            continue\n",
        "\n",
        "        a, b = uniq[0], uniq[1]\n",
        "        if target_model not in (a.model_name, b.model_name):\n",
        "            continue\n",
        "        if random.random() < 0.5:\n",
        "            a, b = b, a\n",
        "\n",
        "        user_text = render_self_recognition_prompt(prompt=a.prompt, code1=a.generated_code, code2=b.generated_code)\n",
        "        label_id = 0 if target_model == a.model_name else 1\n",
        "        examples.append((user_text, label_id))\n",
        "    return examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build dataset\n",
        "rows = []\n",
        "for jsonl_file in INPUT_JSONL_FILES:\n",
        "    rows.extend(list(iter_generations(Path(jsonl_file), dataset_filter=DATASET_FILTER)))\n",
        "print(f\"Loaded {len(rows)} generation rows from {len(INPUT_JSONL_FILES)} files\")\n",
        "\n",
        "examples = build_pairs_for_target(rows, target_model=TARGET_MODEL)\n",
        "print(f\"Prepared {len(examples)} labeled pairs\")\n",
        "\n",
        "records = [\n",
        "    {\"prompt\": text, \"label_id\": int(lbl), \"answer_text\": (\"1\" if int(lbl) == 0 else \"2\")} \n",
        "    for text, lbl in examples\n",
        "]\n",
        "\n",
        "full_ds = Dataset.from_list(records).shuffle(seed=SEED)\n",
        "train_size = int(len(full_ds) * TRAIN_RATIO)\n",
        "train_ds = full_ds.select(range(train_size))\n",
        "eval_ds = full_ds.select(range(train_size, len(full_ds)))\n",
        "len(train_ds), len(eval_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenizer and collator\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_example(ex: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    prompt_text = (ex.get(\"prompt\") or \"\").rstrip() + \"\\n\"\n",
        "    answer_text = (ex.get(\"answer_text\") or \"\").strip()\n",
        "    prompt_enc = tokenizer(\n",
        "        prompt_text,\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "    )\n",
        "    full_enc = tokenizer(\n",
        "        prompt_text + answer_text,\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "    )\n",
        "    input_ids = full_enc[\"input_ids\"]\n",
        "    attention_mask = full_enc[\"attention_mask\"]\n",
        "    labels = [-100] * len(input_ids)\n",
        "    boundary = len(prompt_enc[\"input_ids\"])  # mask prompt, learn on answer tokens only\n",
        "    for i in range(boundary, len(input_ids)):\n",
        "        labels[i] = input_ids[i]\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "train_tok = train_ds.map(tokenize_example, remove_columns=train_ds.column_names)\n",
        "eval_tok = eval_ds.map(tokenize_example, remove_columns=eval_ds.column_names)\n",
        "\n",
        "import torch\n",
        "\n",
        "def data_collator(features: List[Dict[str, Any]]):\n",
        "    batch = tokenizer.pad(features, padding=True, return_tensors=\"pt\")\n",
        "    if \"labels\" in features[0]:\n",
        "        max_len = int(batch[\"input_ids\"].shape[1])\n",
        "        padded_labels: List[List[int]] = []\n",
        "        for f in features:\n",
        "            lbl = list(f.get(\"labels\", []))\n",
        "            if len(lbl) < max_len:\n",
        "                lbl = lbl + ([-100] * (max_len - len(lbl)))\n",
        "            else:\n",
        "                lbl = lbl[:max_len]\n",
        "            padded_labels.append(lbl)\n",
        "        batch[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
        "    return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train\n",
        "output_dir = Path(OUTPUT_DIR)\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
        "\n",
        "# Minimal args for broad transformers version compatibility\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(output_dir),\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=eval_tok if len(eval_tok) > 0 else None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate after training if eval set exists\n",
        "if len(eval_tok) > 0:\n",
        "    metrics = trainer.evaluate()\n",
        "    print(\"Eval metrics:\", metrics)\n",
        "\n",
        "trainer.save_model(str(output_dir))\n",
        "tokenizer.save_pretrained(str(output_dir))\n",
        "print(\"Saved fine-tuned judge ->\", output_dir)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
