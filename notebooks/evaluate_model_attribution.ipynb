{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data root: /Users/ehsan/CursorProjects/llm-collusion/data/model_attribution\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from glob import glob\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "BASE_DIR = \"/Users/ehsan/CursorProjects/llm-collusion\"\n",
        "DATA_ROOT = os.path.join(BASE_DIR, \"data\", \"full_attribution\")\n",
        "\n",
        "print(\"Data root:\", DATA_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_jsonl(path: str) -> List[Dict]:\n",
        "    records: List[Dict] = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            records.append(json.loads(line))\n",
        "    return records\n",
        "\n",
        "\n",
        "def extract_labels(records: List[Dict]) -> Tuple[List[int], List[int], List[str]]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      y_true, y_pred as 0/1 labels over all code items (Code1, Code2 per row),\n",
        "      and class_names [model1, model2]\n",
        "    Skips rows where predicted_attribution is None.\n",
        "    \"\"\"\n",
        "    y_true: List[int] = []\n",
        "    y_pred: List[int] = []\n",
        "    class_names: List[str] = []\n",
        "\n",
        "    if not records:\n",
        "        return y_true, y_pred, class_names\n",
        "\n",
        "    # Use per-file model1/model2 from any row that contains them\n",
        "    for r in records:\n",
        "        m1 = r.get(\"model1\")\n",
        "        m2 = r.get(\"model2\")\n",
        "        if m1 and m2:\n",
        "            class_names = [m1, m2]\n",
        "            break\n",
        "\n",
        "    if not class_names:\n",
        "        return y_true, y_pred, class_names\n",
        "\n",
        "    model1, model2 = class_names\n",
        "\n",
        "    for r in records:\n",
        "        gold = r.get(\"gold_attribution\")\n",
        "        pred = r.get(\"predicted_attribution\")\n",
        "        if not gold or not pred:\n",
        "            # skip if prediction is missing\n",
        "            continue\n",
        "\n",
        "        # Code1\n",
        "        g1 = gold.get(\"Code1\")\n",
        "        p1 = pred.get(\"Code1\")\n",
        "        if g1 in (model1, model2) and p1 in (model1, model2):\n",
        "            y_true.append(0 if g1 == model1 else 1)\n",
        "            y_pred.append(0 if p1 == model1 else 1)\n",
        "\n",
        "        # Code2\n",
        "        g2 = gold.get(\"Code2\")\n",
        "        p2 = pred.get(\"Code2\")\n",
        "        if g2 in (model1, model2) and p2 in (model1, model2):\n",
        "            y_true.append(0 if g2 == model1 else 1)\n",
        "            y_pred.append(0 if p2 == model1 else 1)\n",
        "\n",
        "    return y_true, y_pred, class_names\n",
        "\n",
        "\n",
        "def compute_metrics(y_true: List[int], y_pred: List[int]) -> Dict[str, float]:\n",
        "    if len(y_true) == 0:\n",
        "        return {\"accuracy\": np.nan, \"macro_f1\": np.nan, \"macro_recall\": np.nan}\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=[0, 1], average=None, zero_division=0\n",
        "    )\n",
        "    macro_f1 = float(np.mean(f1))\n",
        "    macro_recall = float(np.mean(recall))\n",
        "    return {\n",
        "        \"accuracy\": float(acc),\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"macro_recall\": macro_recall,\n",
        "        \"support\": int(sum(support)),\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_file(path: str) -> Dict:\n",
        "    records = load_jsonl(path)\n",
        "    y_true, y_pred, class_names = extract_labels(records)\n",
        "    metrics = compute_metrics(y_true, y_pred)\n",
        "\n",
        "    # derive judge and counts\n",
        "    judge = None\n",
        "    for r in records:\n",
        "        jm = r.get(\"judge_model\")\n",
        "        if jm:\n",
        "            judge = jm\n",
        "            break\n",
        "    rows = len(records)\n",
        "    skipped_rows = sum(1 for r in records if not r.get(\"predicted_attribution\"))\n",
        "\n",
        "    return {\n",
        "        \"file\": os.path.relpath(path, BASE_DIR),\n",
        "        \"judge\": judge,\n",
        "        \"model1\": class_names[0] if class_names else None,\n",
        "        \"model2\": class_names[1] if class_names else None,\n",
        "        \"rows\": rows,\n",
        "        \"skipped_rows\": skipped_rows,\n",
        "        **metrics,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2 files\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>judge</th>\n",
              "      <th>model1</th>\n",
              "      <th>model2</th>\n",
              "      <th>rows</th>\n",
              "      <th>skipped_rows</th>\n",
              "      <th>support</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro_f1</th>\n",
              "      <th>macro_recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/model_attribution/mbpp-sanitized/test/jud...</td>\n",
              "      <td>openai/gpt-5</td>\n",
              "      <td>anthropic/claude-haiku-4.5</td>\n",
              "      <td>deepseek/deepseek-chat-v3-0324</td>\n",
              "      <td>257</td>\n",
              "      <td>4</td>\n",
              "      <td>506</td>\n",
              "      <td>0.644269</td>\n",
              "      <td>0.644269</td>\n",
              "      <td>0.644269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/model_attribution/mbpp-sanitized/test/jud...</td>\n",
              "      <td>openai/gpt-5</td>\n",
              "      <td>openai/gpt-5</td>\n",
              "      <td>anthropic/claude-haiku-4.5</td>\n",
              "      <td>257</td>\n",
              "      <td>2</td>\n",
              "      <td>510</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                file         judge  \\\n",
              "0  data/model_attribution/mbpp-sanitized/test/jud...  openai/gpt-5   \n",
              "1  data/model_attribution/mbpp-sanitized/test/jud...  openai/gpt-5   \n",
              "\n",
              "                       model1                          model2  rows  \\\n",
              "0  anthropic/claude-haiku-4.5  deepseek/deepseek-chat-v3-0324   257   \n",
              "1                openai/gpt-5      anthropic/claude-haiku-4.5   257   \n",
              "\n",
              "   skipped_rows  support  accuracy  macro_f1  macro_recall  \n",
              "0             4      506  0.644269  0.644269      0.644269  \n",
              "1             2      510  0.666667  0.666667      0.666667  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find all jsonl files recursively under data/full_attribution\n",
        "all_files = sorted(glob(os.path.join(DATA_ROOT, \"**\", \"*.jsonl\"), recursive=True))\n",
        "print(f\"Found {len(all_files)} files\")\n",
        "\n",
        "rows = []\n",
        "for fp in all_files:\n",
        "    rows.append(evaluate_file(fp))\n",
        "\n",
        "results_df = pd.DataFrame(rows)\n",
        "\n",
        "# Parse benchmark/split from path\n",
        "def parse_path_info(relpath: str) -> Dict[str, str]:\n",
        "    parts = relpath.split(\"/\")\n",
        "    meta = {\"split\": None, \"benchmark\": None}\n",
        "    try:\n",
        "        # Look for \"full_attribution\" in path\n",
        "        if \"full_attribution\" in parts:\n",
        "            idx = parts.index(\"full_attribution\")\n",
        "            meta[\"benchmark\"] = parts[idx + 1] if idx + 1 < len(parts) else None\n",
        "            meta[\"split\"] = parts[idx + 2] if idx + 2 < len(parts) else None\n",
        "    except Exception:\n",
        "        pass\n",
        "    return meta\n",
        "\n",
        "path_info = [parse_path_info(f) for f in results_df[\"file\"].tolist()]\n",
        "results_df = pd.concat([results_df, pd.DataFrame(path_info)], axis=1)\n",
        "\n",
        "# order columns for readability\n",
        "cols = [\n",
        "    \"file\", \"benchmark\", \"split\", \"judge\", \"model1\", \"model2\",\n",
        "    \"rows\", \"skipped_rows\", \"support\", \"accuracy\", \"macro_f1\", \"macro_recall\"\n",
        "]\n",
        "existing_cols = [c for c in cols if c in results_df.columns]\n",
        "results_df = results_df[existing_cols]\n",
        "\n",
        "# sort by benchmark, split, judge, model1, model2\n",
        "results_df = results_df.sort_values(by=[\"benchmark\", \"split\", \"judge\", \"model1\", \"model2\"]).reset_index(drop=True)\n",
        "results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>benchmark</th>\n",
              "      <th>split</th>\n",
              "      <th>judge</th>\n",
              "      <th>model1</th>\n",
              "      <th>model2</th>\n",
              "      <th>rows</th>\n",
              "      <th>skipped_rows</th>\n",
              "      <th>support</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro_f1</th>\n",
              "      <th>macro_recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mbpp-sanitized</td>\n",
              "      <td>test</td>\n",
              "      <td>openai/gpt-5</td>\n",
              "      <td>anthropic/claude-haiku-4.5</td>\n",
              "      <td>deepseek/deepseek-chat-v3-0324</td>\n",
              "      <td>257</td>\n",
              "      <td>4</td>\n",
              "      <td>506</td>\n",
              "      <td>0.644269</td>\n",
              "      <td>0.644269</td>\n",
              "      <td>0.644269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mbpp-sanitized</td>\n",
              "      <td>test</td>\n",
              "      <td>openai/gpt-5</td>\n",
              "      <td>openai/gpt-5</td>\n",
              "      <td>anthropic/claude-haiku-4.5</td>\n",
              "      <td>257</td>\n",
              "      <td>2</td>\n",
              "      <td>510</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        benchmark split         judge                      model1  \\\n",
              "0  mbpp-sanitized  test  openai/gpt-5  anthropic/claude-haiku-4.5   \n",
              "1  mbpp-sanitized  test  openai/gpt-5                openai/gpt-5   \n",
              "\n",
              "                           model2  rows  skipped_rows  support  accuracy  \\\n",
              "0  deepseek/deepseek-chat-v3-0324   257             4      506  0.644269   \n",
              "1      anthropic/claude-haiku-4.5   257             2      510  0.666667   \n",
              "\n",
              "   macro_f1  macro_recall  \n",
              "0  0.644269      0.644269  \n",
              "1  0.666667      0.666667  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Aggregate by (benchmark, split, judge, model pair)\n",
        "agg = (\n",
        "    results_df.groupby([\"benchmark\", \"split\", \"judge\", \"model1\", \"model2\"], dropna=False)\n",
        "    .agg({\n",
        "        \"rows\": \"sum\",\n",
        "        \"skipped_rows\": \"sum\",\n",
        "        \"support\": \"sum\",\n",
        "        \"accuracy\": \"mean\",\n",
        "        \"macro_f1\": \"mean\",\n",
        "        \"macro_recall\": \"mean\",\n",
        "    })\n",
        "    .reset_index()\n",
        "    .sort_values([\"benchmark\", \"split\", \"judge\", \"model1\", \"model2\"]) \n",
        ")\n",
        "agg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook computes full attribution metrics for all files under `data/full_attribution/`.\n",
        "\n",
        "**Extracted Information:**\n",
        "- `benchmark`: Dataset (e.g., mbpp-sanitized)\n",
        "- `split`: Train/validation/test split\n",
        "- `judge`: Judge model used for attribution (from JSONL records)\n",
        "- `model1`, `model2`: The two models being classified\n",
        "- `rows`: Total number of tasks in the file\n",
        "- `skipped_rows`: Number of tasks where prediction is null/missing\n",
        "- `support`: Number of code items (Code1, Code2) used for metrics (excludes skipped)\n",
        "\n",
        "**Metrics:**\n",
        "- `accuracy`: Fraction of correctly attributed code items\n",
        "- `macro_f1`: F1 score averaged over both model classes\n",
        "- `macro_recall`: Recall averaged over both model classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
