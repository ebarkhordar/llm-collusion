{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Judgment Files Analysis\n",
        "\n",
        "This notebook reads judgment JSONL files (e.g., `judge-openai-gpt-5_classify-deepseek-deepseek-chat-v3-0324_vs_anthropic-claude-haiku-4.5.jsonl`) and similar under `data/`, computes accuracy, precision, recall, F1, and provides per-file and aggregate insights, including which task IDs failed with predicted vs gold attributions.\n",
        "\n",
        "- Works across multiple directories (e.g., `data/model_attribution/...`)\n",
        "- Handles missing predictions (`predicted_attribution: null`)\n",
        "- Summarizes per-file metrics and aggregates across all selected files\n",
        "- Lists hardest tasks (most frequently misattributed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os\n",
        "import json\n",
        "from glob import glob\n",
        "from dataclasses import dataclass\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration\n",
        "# You can change this to any glob that matches judgment files\n",
        "# Examples:\n",
        "# - 'data/model_attribution/mbpp-sanitized/test/judge-*.jsonl'\n",
        "# - 'data/model_attribution/**/test/judge-*.jsonl'\n",
        "# - 'data/**/judge-*.jsonl'\n",
        "FILE_GLOB = 'data/model_attribution/**/test/judge-*.jsonl'\n",
        "\n",
        "# Minimum rows to include a file in metrics (to filter out empty or degenerate files)\n",
        "MIN_ROWS_PER_FILE = 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_jsonl(path: str) -> List[Dict[str, Any]]:\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                rows.append(json.loads(line))\n",
        "            except json.JSONDecodeError:\n",
        "                # skip malformed lines\n",
        "                continue\n",
        "    return rows\n",
        "\n",
        "\n",
        "def load_files(file_glob: str) -> Dict[str, pd.DataFrame]:\n",
        "    files = sorted(glob(file_glob, recursive=True))\n",
        "    dataframes: Dict[str, pd.DataFrame] = {}\n",
        "    for fp in files:\n",
        "        rows = read_jsonl(fp)\n",
        "        if not rows:\n",
        "            continue\n",
        "        df = pd.DataFrame(rows)\n",
        "        if len(df) >= MIN_ROWS_PER_FILE:\n",
        "            dataframes[fp] = df\n",
        "    return dataframes\n",
        "\n",
        "\n",
        "def normalize_record(row: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    # Flatten predicted and gold attributions\n",
        "    pred = row.get('predicted_attribution') or {}\n",
        "    gold = row.get('gold_attribution') or {}\n",
        "    rec = dict(row)\n",
        "    rec['pred_Code1'] = pred.get('Code1')\n",
        "    rec['pred_Code2'] = pred.get('Code2')\n",
        "    rec['gold_Code1'] = gold.get('Code1')\n",
        "    rec['gold_Code2'] = gold.get('Code2')\n",
        "    # Ensure is_correct as boolean or None\n",
        "    ic = row.get('is_correct')\n",
        "    rec['is_correct'] = (bool(ic) if ic is not None else None)\n",
        "    return rec\n",
        "\n",
        "\n",
        "def normalize_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df.empty:\n",
        "        return df\n",
        "    norm = df.apply(lambda r: pd.Series(normalize_record(r)), axis=1)\n",
        "    # Preserve original columns if needed\n",
        "    for c in df.columns:\n",
        "        if c not in norm.columns:\n",
        "            norm[c] = df[c]\n",
        "    return norm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def binary_metrics_from_correct(series: pd.Series) -> Dict[str, Any]:\n",
        "    # Consider 'positive' = correct prediction; ignore None\n",
        "    vals = [v for v in series.tolist() if v is not None]\n",
        "    if not vals:\n",
        "        return {\n",
        "            'n': 0,\n",
        "            'accuracy': None,\n",
        "            'precision': None,\n",
        "            'recall': None,\n",
        "            'f1': None,\n",
        "            'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0,\n",
        "        }\n",
        "    n = len(vals)\n",
        "    tp = sum(1 for v in vals if v is True)\n",
        "    fn = 0  # with only correctness known, can't separate TN/FP meaningfully\n",
        "    tn = 0\n",
        "    fp = sum(1 for v in vals if v is False)\n",
        "    accuracy = tp / n if n else None\n",
        "    precision = tp / (tp + fp) if (tp + fp) else None\n",
        "    recall = tp / (tp + fn) if (tp + fn) else None\n",
        "    f1 = (2 * precision * recall / (precision + recall)) if (precision and recall and (precision + recall)) else None\n",
        "    return {'n': n, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn}\n",
        "\n",
        "\n",
        "def per_position_accuracy(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    # Accuracy for Code1 and Code2 labels separately\n",
        "    mask = df['gold_Code1'].notna() & df['gold_Code2'].notna()\n",
        "    df2 = df[mask].copy()\n",
        "    if df2.empty:\n",
        "        return {'code1_acc': None, 'code2_acc': None}\n",
        "    code1_acc = (df2['pred_Code1'] == df2['gold_Code1']).mean() if 'pred_Code1' in df2 else None\n",
        "    code2_acc = (df2['pred_Code2'] == df2['gold_Code2']).mean() if 'pred_Code2' in df2 else None\n",
        "    return {'code1_acc': float(code1_acc) if code1_acc is not None else None,\n",
        "            'code2_acc': float(code2_acc) if code2_acc is not None else None}\n",
        "\n",
        "\n",
        "def confusion_counts(df: pd.DataFrame, col_pred: str, col_gold: str) -> pd.DataFrame:\n",
        "    # Confusion across labels present in file (usually two models)\n",
        "    if col_pred not in df or col_gold not in df:\n",
        "        return pd.DataFrame()\n",
        "    dd = df.dropna(subset=[col_pred, col_gold])\n",
        "    if dd.empty:\n",
        "        return pd.DataFrame()\n",
        "    labels = sorted(set(dd[col_gold].unique()).union(set(dd[col_pred].unique())))\n",
        "    mat = pd.DataFrame(0, index=labels, columns=labels)\n",
        "    for _, r in dd.iterrows():\n",
        "        mat.loc[r[col_gold], r[col_pred]] += 1\n",
        "    mat.index.name = 'gold'\n",
        "    mat.columns.name = 'pred'\n",
        "    return mat\n",
        "\n",
        "\n",
        "def summarize_file(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    df_norm = normalize_dataframe(df)\n",
        "    metrics = binary_metrics_from_correct(df_norm['is_correct']) if 'is_correct' in df_norm else {}\n",
        "    pos_acc = per_position_accuracy(df_norm)\n",
        "    conf1 = confusion_counts(df_norm, 'pred_Code1', 'gold_Code1')\n",
        "    conf2 = confusion_counts(df_norm, 'pred_Code2', 'gold_Code2')\n",
        "    failed = df_norm[df_norm['is_correct'] == False]\n",
        "    failed_view = failed[['benchmark', 'task_id', 'model1', 'model2', 'pred_Code1', 'pred_Code2', 'gold_Code1', 'gold_Code2']].copy() if not failed.empty else pd.DataFrame()\n",
        "    return {\n",
        "        'n_rows': int(len(df_norm)),\n",
        "        'metrics': metrics,\n",
        "        'per_position': pos_acc,\n",
        "        'confusion_code1': conf1,\n",
        "        'confusion_code2': conf2,\n",
        "        'failed': failed_view,\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "files_to_dfs = load_files(FILE_GLOB)\n",
        "print(f\"Found {len(files_to_dfs)} files for glob: {FILE_GLOB}\")\n",
        "list(files_to_dfs.keys())[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "per_file_summaries: Dict[str, Dict[str, Any]] = {}\n",
        "for fp, df in files_to_dfs.items():\n",
        "    per_file_summaries[fp] = summarize_file(df)\n",
        "\n",
        "# Create a compact table of per-file metrics\n",
        "rows = []\n",
        "for fp, summ in per_file_summaries.items():\n",
        "    m = summ['metrics']\n",
        "    pos = summ['per_position']\n",
        "    rows.append({\n",
        "        'file': fp,\n",
        "        'n_rows': summ['n_rows'],\n",
        "        'accuracy': m.get('accuracy'),\n",
        "        'precision': m.get('precision'),\n",
        "        'recall': m.get('recall'),\n",
        "        'f1': m.get('f1'),\n",
        "        'code1_acc': pos.get('code1_acc'),\n",
        "        'code2_acc': pos.get('code2_acc'),\n",
        "    })\n",
        "summary_df = pd.DataFrame(rows).sort_values(by=['f1', 'accuracy'], ascending=[False, False])\n",
        "summary_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate across all files\n",
        "all_df_list = []\n",
        "for df in files_to_dfs.values():\n",
        "    all_df_list.append(normalize_dataframe(df))\n",
        "\n",
        "all_df = pd.concat(all_df_list, ignore_index=True) if all_df_list else pd.DataFrame()\n",
        "print(f\"Total rows across files: {len(all_df)}\")\n",
        "\n",
        "agg = summarize_file(all_df) if not all_df.empty else None\n",
        "agg_metrics = agg['metrics'] if agg else {}\n",
        "agg_pos = agg['per_position'] if agg else {}\n",
        "print({\n",
        "    'n_rows': len(all_df),\n",
        "    'accuracy': agg_metrics.get('accuracy'),\n",
        "    'precision': agg_metrics.get('precision'),\n",
        "    'recall': agg_metrics.get('recall'),\n",
        "    'f1': agg_metrics.get('f1'),\n",
        "    'code1_acc': agg_pos.get('code1_acc'),\n",
        "    'code2_acc': agg_pos.get('code2_acc'),\n",
        "})\n",
        "\n",
        "# Display confusions if any\n",
        "agg_conf1 = agg['confusion_code1'] if agg else pd.DataFrame()\n",
        "agg_conf2 = agg['confusion_code2'] if agg else pd.DataFrame()\n",
        "agg_conf1, agg_conf2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hardest tasks: which task_ids fail most often\n",
        "if agg and not agg['failed'].empty:\n",
        "    fails = agg['failed']\n",
        "    # Count failures by task_id\n",
        "    counts = fails.groupby('task_id').size().sort_values(ascending=False).rename('fail_count')\n",
        "    hardest = counts.to_frame().reset_index()\n",
        "    display(hardest.head(20))\n",
        "else:\n",
        "    print('No failures found or no data loaded.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample of failed cases with details\n",
        "if agg and not agg['failed'].empty:\n",
        "    display(agg['failed'].head(30))\n",
        "else:\n",
        "    print('No failed cases to display.')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
